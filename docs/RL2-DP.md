# Bellman equations, characterizing optimal policies

* [Home](https://supaerodatascience.github.io/reinforcement-learning/)
* [Github repository](https://github.com/SupaeroDataScience/reinforcement-learning/)

The previous class (RL1) introduced the model of Markov Decision Processes as a way to describe discrete-time, stochastic, dynamical systems. Our focus is on controling such systems. For this we want to characterize what makes a policy optimal and how to find it. This class covers the model-based resolution of MDPs, in particular via Dynamic Programming.

During class we will cover sections 1 to 5 of the notebook. Sections 6 and 7 are extra content that is important for a better understanding of the concepts at stake but will not be covered in class and will not be directly reused in future classes.

[Notebook](https://github.com/SupaeroDataScience/reinforcement-learning/blob/master/notebooks/RL2%20-%20Bellman%20equations%2C%20characterizing%20optimal%20policies.ipynb)

